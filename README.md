# Классификация пар предложений: Contradictory, My Dear Watson

Этот репозиторий содержит решение для соревнования Kaggle [Contradictory, My Dear Watson](https://www.kaggle.com/competitions/contradictory-my-dear-watson), целью которого является классификация пар предложений (премиса и гипотеза) на три категории: entailment (0), neutral (1) или contradiction (2). Решение использует предобученную модель XLM-RoBERTa для создания эмбеддингов и дальнейшего fine-tuning.

## Описание проекта

Цель проекта — разработка модели машинного обучения для определения логической связи между парой предложений (премиса и гипотеза) в наборе данных. Задача осложняется многоязычностью данных (включая английский, испанский, французский, китайский и другие языки), что требует использования модели, способной обрабатывать тексты на разных языках. В данном решении используется XLM-RoBERTa-base, которая токенизирует пары предложений и выполняет их классификацию.

Итоговая метрика accuracy на тестовом наборе составляет **0.71222**, что обусловлено ограниченным количеством эпох обучения (всего 3), которая в свою очередь является следствием ограниченных вычислительных ресурсов. Для улучшения результата рекомендуется увеличить количество эпох.

## Содержимое репозитория

| Файл/Папка | Описание |
|------------|----------|
| `processing.ipynb`                   | Основной скрипт для обучения модели XLM-RoBERTa и создания предсказаний.                   |
| `train.csv`                          | Обучающий датасет                                                                          |
| `test.csv`                           | Тестовый датасет                                                                           |
| `submission.csv`                     | Файл с предсказаниями для тестового набора                                                 |
| `Tokenizing/`                        | Папка с файлом токенизации и сжатыми данными токенов                                       |
| `Tokenizing/Tokenizing.py`           | Скрипт для токенизации `train.csv` и `test.csv` с использованием токенизатора XLM-RoBERTa. |
| `Tokenizing/train_data.pt.gz`        | Сжатый файл с токенизированными данными для обучающего набора.                             |
| `Tokenizing/test_data.pt.gz`         | Сжатый файл с токенизированными данными для тестового набора.                              |

## Использование

Папка `Tokenizing/` не нужна при использовании, сделана для логирования. При запуске убедитесь, что датасеты лежат в одной директории с `processing.ipynb`.

### Требования
- Установленные библиотеки: `transformers`, `torch`, `pandas`, `numpy`.

## Данные

Данные предоставлены в рамках соревнования [Contradictory, My Dear Watson](https://www.kaggle.com/competitions/contradictory-my-dear-watson):

- **`train.csv`**:
  - Столбцы: `id`, `premise`, `hypothesis`, `label`, `language`.
  - `label`: 0 (entailment), 1 (neutral), 2 (contradiction).
  - Размер: 12,120 записей.
  - Многоязычные данные (английский, испанский, французский и др.).

- **`test.csv`**:
  - Столбцы: `id`, `premise`, `hypothesis`, `language`.
  - Размер: 5,195 записей.

- **`train_data.pt`, test_data.pt`**:
  - Токенизированные данные (PyTorch тензоры) с `input_ids`, `attention_mask` и (для `train_data.pt`) `labels`.

- **`submission_new.csv`**:
  - Столбцы: `id`, `target` (значения: 0, 1, 2).

## Модель

Решение использует предобученную модель [XLM-RoBERTa-base](https://huggingface.co/xlm-roberta-base) для задачи классификации последовательностей с настройкой `num_labels=3`. Параметры обучения:
- **Оптимизатор**: AdamW (learning rate = 2e-5, weight decay = 0.01).
- **Размер батча**: 32.
- **Количество эпох**: 3.
- **Планировщик**: Linear schedule с warmup (10% от общего числа шагов).

Модель обрабатывает токенизированные пары премиса-гипотеза и выдаёт вероятности для трёх классов.

## Результаты

Текущая реализация достигла **accuracy 0.71222** на тестовом наборе. Этот результат ограничен малым количеством эпох (3), которые всё равно заняли 56 минут обучения. Для улучшения производительности рекомендуется:
- Увеличить количество эпох (например, до 5–10).
- Добавить валидационную выборку для мониторинга обобщающей способности.
- Экспериментировать с гиперпараметрами (learning rate, batch size).

## Примечания

- **Gzip-архивы**: Токенизированные данные хранятся в сжатом виде для экономии места. Используйте `gunzip` для распаковки.

## Автор

- **Артём Сабитов** (GitHub: [sabitovartyom](https://github.com/sabitovartyom))
